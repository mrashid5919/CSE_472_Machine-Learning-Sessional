{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "#Define transformation\n",
    "transform=transforms.ToTensor()\n",
    "\n",
    "#Load the training dataset\n",
    "train_dataset=datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "\n",
    "#Load test dataset seperately\n",
    "with open('b2.pkl', 'rb') as b2:\n",
    "  test_dataset = pickle.load(b2)\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Define sizes for train and validation splits\n",
    "train_size = int(0.8 * len(train_dataset))  # 80% for training\n",
    "val_size = len(train_dataset) - train_size  # 20% for validation\n",
    "\n",
    "# Split the dataset\n",
    "train_subset, val_subset = random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x0000028F98D95990>\n"
     ]
    }
   ],
   "source": [
    "# Dataloader for mini-batch processing\n",
    "import torch\n",
    "# Create DataLoader for the training set\n",
    "trainloader = torch.utils.data.DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Create DataLoader for the validation set\n",
    "valloader = torch.utils.data.DataLoader(val_subset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Test DataLoader (for independent test set)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Initializing weights and biases\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        self.biases = np.zeros((1, output_dim))\n",
    "\n",
    "    def forward(self, inputs, training=True):\n",
    "        # Store inputs for use in backpropagation\n",
    "        self.inputs = inputs\n",
    "        # Linear transformation\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_out, learning_rate):\n",
    "        # Gradient of weights and biases\n",
    "        self.d_weights = np.dot(self.inputs.T, d_out)\n",
    "        self.d_biases = np.sum(d_out, axis=0, keepdims=True)\n",
    "        # Gradient of inputs\n",
    "        self.d_inputs = np.dot(d_out, self.weights.T)\n",
    "        # Update weights and biases\n",
    "        self.weights -= learning_rate * self.d_weights\n",
    "        self.biases -= learning_rate * self.d_biases\n",
    "        return self.d_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, dim, epsilon=1e-5, momentum=0.9):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.gamma = np.ones((1, dim))\n",
    "        self.beta = np.zeros((1, dim))\n",
    "        self.running_mean = np.zeros((1, dim))\n",
    "        self.running_var = np.ones((1, dim))\n",
    "\n",
    "    def forward(self, inputs, training=True):\n",
    "        self.inputs = inputs\n",
    "        if training:\n",
    "            # Calculate mean and variance for the batch\n",
    "            self.batch_mean = np.mean(inputs, axis=0)\n",
    "            self.batch_var = np.var(inputs, axis=0)\n",
    "\n",
    "            # Normalize the batch\n",
    "            self.normalized = (inputs - self.batch_mean) / np.sqrt(self.batch_var + self.epsilon)\n",
    "\n",
    "            # Scale and shift\n",
    "            self.output = self.gamma * self.normalized + self.beta\n",
    "\n",
    "            # Update running statistics\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.batch_mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.batch_var\n",
    "        else:\n",
    "            # Use running mean and variance for inference\n",
    "            self.normalized = (inputs - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "            self.output = self.gamma * self.normalized + self.beta\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_out, learning_rate):\n",
    "        # Gradient of beta and gamma\n",
    "        self.d_beta = np.sum(d_out, axis=0)\n",
    "        self.d_gamma = np.sum(d_out * self.normalized, axis=0)\n",
    "\n",
    "        # Gradient of normalized input\n",
    "        d_normalized = d_out * self.gamma\n",
    "\n",
    "        # Gradient of variance\n",
    "        d_var = np.sum(d_normalized * (self.inputs - self.batch_mean) * -0.5 * (self.batch_var + self.epsilon) ** (-1.5), axis=0)\n",
    "\n",
    "        # Gradient of mean\n",
    "        d_mean = np.sum(d_normalized * -1 / np.sqrt(self.batch_var + self.epsilon), axis=0) + d_var * np.mean(-2 * (self.inputs - self.batch_mean), axis=0)\n",
    "\n",
    "        # Gradient of inputs\n",
    "        d_inputs = d_normalized / np.sqrt(self.batch_var + self.epsilon) + d_var * 2 * (self.inputs - self.batch_mean) / d_out.shape[0] + d_mean / d_out.shape[0]\n",
    "\n",
    "        # Update gamma and beta\n",
    "        self.gamma -= learning_rate * self.d_gamma\n",
    "        self.beta -= learning_rate * self.d_beta\n",
    "\n",
    "        return d_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, inputs, training=True):\n",
    "        self.inputs = inputs\n",
    "        return np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        d_out[self.inputs <= 0] = 0\n",
    "        return d_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, inputs, training=True):\n",
    "        # Ensure numerical stability with max subtraction\n",
    "        exps = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, inputs, training=True):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, BatchNormalization):\n",
    "                inputs = layer.forward(inputs, training)\n",
    "            else:\n",
    "                inputs = layer.forward(inputs)\n",
    "        return inputs\n",
    "\n",
    "    def backward(self, loss_grad, learning_rate):\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer, (DenseLayer, BatchNormalization)):\n",
    "                loss_grad = layer.backward(loss_grad, learning_rate)\n",
    "            else:\n",
    "                loss_grad = layer.backward(loss_grad)\n",
    "\n",
    "    # Method to extract weights\n",
    "    def get_weights(self):\n",
    "        weights = []\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'biases'):\n",
    "                weights.append((layer.weights, layer.biases))\n",
    "            elif hasattr(layer, 'gamma'):\n",
    "                weights.append((layer.gamma, layer.beta,layer.running_mean,layer.running_var))\n",
    "        return weights\n",
    "\n",
    "    # Method to set weights\n",
    "    def set_weights(self, weights):\n",
    "        idx = 0\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'biases'):\n",
    "                layer.weights, layer.biases = weights[idx]\n",
    "                idx += 1\n",
    "            elif hasattr(layer, 'gamma'):\n",
    "                layer.gamma, layer.beta, layer.running_mean, layer.running_var = weights[idx]\n",
    "                idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "\n",
    "    def forward(self, inputs, training=True):\n",
    "        if training:\n",
    "            self.mask = (np.random.rand(*inputs.shape) > self.rate) / (1 - self.rate)\n",
    "            return inputs * self.mask\n",
    "        return inputs\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        return d_out * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.lr = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m_t = 0\n",
    "        self.v_t = 0\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, weights, d_weights):\n",
    "        self.t += 1\n",
    "        self.m_t = self.beta1 * self.m_t + (1 - self.beta1) * d_weights\n",
    "        self.v_t = self.beta2 * self.v_t + (1 - self.beta2) * (d_weights ** 2)\n",
    "        m_hat = self.m_t / (1 - self.beta1 ** self.t)\n",
    "        v_hat = self.v_t / (1 - self.beta2 ** self.t)\n",
    "        weights -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "        return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(predictions, labels):\n",
    "    # Ensure numerical stability with log-sum-exp trick\n",
    "    exps = np.exp(predictions - np.max(predictions, axis=1, keepdims=True))\n",
    "    softmax = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "    # Loss calculation\n",
    "    loss = -np.sum(labels * np.log(softmax + 1e-8)) / predictions.shape[0]\n",
    "    return loss, softmax\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    return np.mean(preds == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    #model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.view(inputs.shape[0], -1).numpy()\n",
    "            labels_onehot = np.eye(10)[labels.numpy()]\n",
    "            ss=MinMaxScaler()\n",
    "            inputs=ss.fit_transform(inputs)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model.forward(inputs, training=False)\n",
    "            loss, softmax_outputs = cross_entropy_loss(outputs, labels_onehot)\n",
    "            total_loss += loss\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predictions = np.argmax(softmax_outputs, axis=1)\n",
    "            correct_predictions += np.sum(predictions == labels.numpy())\n",
    "            total_samples += labels.shape[0]\n",
    "\n",
    "            # Store labels and predictions for F1 calculation\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_predictions.extend(predictions)\n",
    "\n",
    "    # Calculate average loss, accuracy, and macro-F1 score\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "    return avg_loss, accuracy, f1\n",
    "\n",
    "def plot_metrics(train_losses, val_losses, train_accuracies, val_accuracies, val_macro_f1_scores):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss vs. Epochs\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot training and validation accuracy\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(epochs, train_accuracies, label=\"Train Accuracy\")\n",
    "    plt.plot(epochs, val_accuracies, label=\"Validation Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy vs. Epochs\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot validation macro-F1 scores\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(epochs, val_macro_f1_scores, label=\"Validation Macro-F1\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Macro-F1 Score\")\n",
    "    plt.title(\"Macro-F1 Score vs. Epochs\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, trainloader, valloader, optimizer, epochs=10, learning_rate=0.001):\n",
    "    # Lists to store metrics for reporting\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    val_macro_f1_scores = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        #model.train()  # Set model to training mode\n",
    "        epoch_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for batch in tqdm(trainloader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.view(inputs.shape[0], -1).numpy()  # Flatten 28x28 images\n",
    "            labels_onehot = np.eye(10)[labels.numpy()]  # One-hot encoding for labels\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model.forward(inputs)\n",
    "            loss, softmax_outputs = cross_entropy_loss(outputs, labels_onehot)\n",
    "            epoch_loss += loss\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predictions = np.argmax(softmax_outputs, axis=1)\n",
    "            correct_predictions += np.sum(predictions == labels.numpy())\n",
    "            total_samples += labels.shape[0]\n",
    "\n",
    "            # Backward pass\n",
    "            loss_grad = softmax_outputs - labels_onehot\n",
    "            model.backward(loss_grad, learning_rate)\n",
    "\n",
    "        # Calculate average training loss and accuracy\n",
    "        train_losses.append(epoch_loss / len(trainloader))\n",
    "        train_accuracies.append(correct_predictions / total_samples)\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_accuracy, val_f1 = evaluate(model, valloader)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_macro_f1_scores.append(val_f1)\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"Training Loss: {train_losses[-1]:.4f}, Training Accuracy: {train_accuracies[-1]:.4f}\")\n",
    "        print(f\"Validation Loss: {val_losses[-1]:.4f}, Validation Accuracy: {val_accuracies[-1]:.4f}, Validation Macro-F1: {val_macro_f1_scores[-1]:.4f}\")\n",
    "\n",
    "    # Plotting metrics\n",
    "    plot_metrics(train_losses, val_losses, train_accuracies, val_accuracies, val_macro_f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamOptimizer(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.003\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Train and evaluate\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[43mtrain\u001b[49m(model, trainloader, valloader, optimizer, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.003\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest avg loss, accuracy, f1 score:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m evaluate(model, testloader)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "input_dim = 28 * 28  # FashionMNIST images are 28x28\n",
    "output_dim = 10      # 10 classes\n",
    "\n",
    "# Set up model layers\n",
    "layers = [\n",
    "    DenseLayer(input_dim, 128),\n",
    "    BatchNormalization(128),\n",
    "    ReLU(),\n",
    "    Dropout(rate=0.2),\n",
    "    DenseLayer(128, 64),\n",
    "    BatchNormalization(64),\n",
    "    ReLU(),\n",
    "    DenseLayer(64, output_dim),\n",
    "    #Softmax()\n",
    "]\n",
    "\n",
    "# Create the model and optimizer\n",
    "model = SimpleNN(layers)\n",
    "optimizer = AdamOptimizer(learning_rate=0.003)\n",
    "\n",
    "# Train and evaluate\n",
    "train(model, trainloader, valloader, optimizer, epochs=10, learning_rate=0.003)\n",
    "print(\"Test avg loss, accuracy, f1 score:\")\n",
    "evaluate(model, testloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save only weights\n",
    "weights = model.get_weights()\n",
    "with open('model_1905103.pickle', 'wb') as file:\n",
    "    pickle.dump(weights, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test avg loss, accuracy, f1 score:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.067663264998905, 0.6973137635004154, 0.27426537225982744)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "input_dim = 28 * 28  \n",
    "output_dim = 10     \n",
    "\n",
    "# Set up model layers\n",
    "layers = [\n",
    "    DenseLayer(input_dim, 128),\n",
    "    BatchNormalization(128),\n",
    "    ReLU(),\n",
    "    Dropout(rate=0.2),\n",
    "    DenseLayer(128, 64),\n",
    "    BatchNormalization(64),\n",
    "    ReLU(),\n",
    "    DenseLayer(64, output_dim),\n",
    "    #Softmax()\n",
    "]\n",
    "model = SimpleNN(layers)\n",
    "\n",
    "# Load weights\n",
    "with open('model_1905103.pickle', 'rb') as file:\n",
    "    weights = pickle.load(file)\n",
    "model.set_weights(weights)\n",
    "\n",
    "#predictions = model.forward(testloader, training=False)\n",
    "print(\"Test avg loss, accuracy, f1 score:\")\n",
    "evaluate(model, testloader)\n",
    "#predicted_labels = np.argmax(predictions, axis=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
